#!/usr/bin/env python3
"""
å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ

Usage:
    python tools/compare_evaluations.py evaluation_results/*/test_metrics.json
    python tools/compare_evaluations.py evaluation_results/run_*/test_metrics.json --output comparison.csv
"""

import argparse
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict
import sys


def load_metrics(json_path: Path) -> Dict:
    """åŠ è½½å•ä¸ªtest_metrics.jsonæ–‡ä»¶"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ {json_path}: {e}")
        return None


def extract_key_metrics(metrics: Dict, json_path: Path) -> Dict:
    """ä»metricsä¸­æå–å…³é”®ä¿¡æ¯ç”¨äºå¯¹æ¯”"""
    try:
        # å…¼å®¹æ–°æ—§æ ¼å¼
        if 'overall_metrics' in metrics:
            # æ–°æ ¼å¼ (v2.0)
            return {
                'run_name': json_path.parent.name,
                'checkpoint': metrics['evaluation_info']['checkpoint_path'],
                'eval_time': metrics['evaluation_info']['evaluation_time'],
                'model': metrics['model_config']['architecture'],
                'dropout': metrics['model_config']['dropout'],
                'best_epoch': metrics['model_config']['best_epoch'],
                'val_mae': metrics['model_config']['val_mae'],
                'age_range': metrics['dataset_config']['age_range'],
                'test_samples': metrics['dataset_config']['total_samples'],
                'test_mae': metrics['overall_metrics']['MAE']['value'],
                'test_rmse': metrics['overall_metrics']['RMSE']['value'],
                'correlation': metrics['overall_metrics']['Correlation']['value'],
                'acc_5y': metrics['overall_metrics']['Accuracy_5years']['value'],
                'acc_10y': metrics['overall_metrics']['Accuracy_10years']['value'],
                'acc_15y': metrics['overall_metrics']['Accuracy_15years']['value'],
                'outlier_pct': metrics.get('error_analysis', {}).get('outlier_count', {}).get('percentage', 0)
            }
        else:
            # æ—§æ ¼å¼ (v1.0)
            return {
                'run_name': json_path.parent.name,
                'checkpoint': 'N/A',
                'eval_time': 'N/A',
                'model': 'N/A',
                'dropout': 'N/A',
                'best_epoch': 'N/A',
                'val_mae': 'N/A',
                'age_range': 'N/A',
                'test_samples': metrics.get('total_samples', 'N/A'),
                'test_mae': metrics['MAE'],
                'test_rmse': metrics['RMSE'],
                'correlation': metrics['Correlation'],
                'acc_5y': metrics['Accuracy_5years'],
                'acc_10y': metrics['Accuracy_10years'],
                'acc_15y': metrics['Accuracy_15years'],
                'outlier_pct': 0
            }
    except KeyError as e:
        print(f"âš ï¸  {json_path.parent.name}: ç¼ºå°‘å­—æ®µ {e}")
        return None


def compare_evaluations(json_paths: List[Path], output_path: Path = None):
    """å¯¹æ¯”å¤šä¸ªè¯„ä¼°ç»“æœ"""
    print(f"\nğŸ“Š å¯¹æ¯” {len(json_paths)} ä¸ªè¯„ä¼°ç»“æœ...\n")
    
    # åŠ è½½æ‰€æœ‰metrics
    all_metrics = []
    for json_path in json_paths:
        metrics = load_metrics(json_path)
        if metrics:
            extracted = extract_key_metrics(metrics, json_path)
            if extracted:
                all_metrics.append(extracted)
    
    if not all_metrics:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
        return
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_metrics)
    
    # æŒ‰MAEæ’åº
    df = df.sort_values('test_mae')
    
    # æ ¼å¼åŒ–æ˜¾ç¤º
    display_df = df.copy()
    for col in ['val_mae', 'test_mae', 'test_rmse', 'correlation', 'outlier_pct']:
        if col in display_df.columns:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.2f}" if isinstance(x, (int, float)) else x)
    
    for col in ['acc_5y', 'acc_10y', 'acc_15y']:
        if col in display_df.columns:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.1f}%" if isinstance(x, (int, float)) else x)
    
    # æ‰“å°æ‘˜è¦è¡¨æ ¼
    print("="*120)
    print("è¯„ä¼°ç»“æœå¯¹æ¯”")
    print("="*120)
    
    summary_cols = ['run_name', 'model', 'dropout', 'age_range', 'test_samples', 
                    'val_mae', 'test_mae', 'test_rmse', 'correlation']
    available_cols = [col for col in summary_cols if col in display_df.columns]
    print(display_df[available_cols].to_string(index=False))
    print("="*120)
    
    print("\nå‡†ç¡®ç‡å¯¹æ¯”:")
    print("-"*80)
    acc_cols = ['run_name', 'acc_5y', 'acc_10y', 'acc_15y', 'outlier_pct']
    available_acc_cols = [col for col in acc_cols if col in display_df.columns]
    print(display_df[available_acc_cols].to_string(index=False))
    print("-"*80)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_idx = df['test_mae'].idxmin()
    best_run = df.loc[best_idx, 'run_name']
    best_mae = df.loc[best_idx, 'test_mae']
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_run}")
    print(f"   Test MAE: {best_mae:.2f} years")
    print(f"   Test RMSE: {df.loc[best_idx, 'test_rmse']:.2f} years")
    print(f"   Correlation: {df.loc[best_idx, 'correlation']:.4f}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if output_path:
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_path}")
    
    # æ‰“å°å¹´é¾„æ®µå¯¹æ¯”ï¼ˆå¯é€‰ï¼‰
    if len(json_paths) <= 5:  # åªå¯¹å°‘é‡æ¨¡å‹æ˜¾ç¤ºå¹´é¾„æ®µå¯¹æ¯”
        print("\nå¹´é¾„æ®µMAEå¯¹æ¯”:")
        print("-"*80)
        for json_path in json_paths:
            metrics = load_metrics(json_path)
            if metrics and 'age_group_analysis' in metrics:
                run_name = json_path.parent.name
                print(f"\n{run_name}:")
                for group in metrics['age_group_analysis']:
                    age_range = group['age_range']
                    mae = group['mae']
                    count = group['count']
                    print(f"  {age_range:>8}: MAE={mae:>6.2f}, n={count:>3}")
        print("-"*80)


def main():
    parser = argparse.ArgumentParser(
        description='å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¯¹æ¯”æ‰€æœ‰è¯„ä¼°ç»“æœ
  python tools/compare_evaluations.py evaluation_results/*/test_metrics.json
  
  # å¯¹æ¯”ç‰¹å®šruns
  python tools/compare_evaluations.py evaluation_results/run_20260113_*/test_metrics.json
  
  # ä¿å­˜å¯¹æ¯”ç»“æœåˆ°CSV
  python tools/compare_evaluations.py evaluation_results/*/test_metrics.json --output comparison.csv
        """
    )
    
    parser.add_argument('json_files', nargs='+', type=str,
                       help='test_metrics.jsonæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒglobæ¨¡å¼ï¼‰')
    parser.add_argument('--output', '-o', type=str, default=None,
                       help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    # æ”¶é›†æ‰€æœ‰JSONæ–‡ä»¶
    json_paths = []
    for pattern in args.json_files:
        path = Path(pattern)
        if path.exists() and path.is_file():
            json_paths.append(path)
        else:
            # å°è¯•globåŒ¹é…
            matched = list(Path('.').glob(pattern))
            json_paths.extend([p for p in matched if p.is_file()])
    
    if not json_paths:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {args.json_files}")
        sys.exit(1)
    
    # å»é‡
    json_paths = list(set(json_paths))
    
    # æ‰§è¡Œå¯¹æ¯”
    output_path = Path(args.output) if args.output else None
    compare_evaluations(json_paths, output_path)


if __name__ == '__main__':
    main()
