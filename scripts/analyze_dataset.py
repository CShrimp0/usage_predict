"""
åˆ†ææ•°æ®é›†ä¸­çš„å›¾åƒåˆ†å¸ƒå’Œç›¸ä¼¼åº¦
æ£€æŸ¥æ¯ä¸ªå—è¯•è€…çš„å›¾åƒæ•°é‡ï¼Œå¹¶è®¡ç®—åŒä¸€å—è¯•è€…ä¸åŒå›¾åƒä¹‹é—´çš„ç›¸ä¼¼åº¦
"""

import os
import numpy as np
import pandas as pd
from pathlib import Path
from PIL import Image
from collections import defaultdict
from skimage.metrics import structural_similarity as ssim
import cv2
from tqdm import tqdm


def analyze_image_distribution(image_dir):
    """
    åˆ†æå›¾åƒåˆ†å¸ƒï¼šç»Ÿè®¡æ¯ä¸ªIDçš„å›¾åƒæ•°é‡
    
    Args:
        image_dir: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        subject_images: {subject_id: [image_paths]}
    """
    image_folder = Path(image_dir)
    subject_images = defaultdict(list)
    
    # æ‰«ææ‰€æœ‰å›¾åƒ
    for img_file in sorted(image_folder.glob('*.png')):
        filename = img_file.stem  # anon_xxx_x
        parts = filename.split('_')
        
        if len(parts) >= 3:
            # æ ¼å¼: anon_xxx_x
            subject_id = parts[1]  # xxx
            sample_num = parts[2]  # x (1/2/3)
            subject_images[subject_id].append({
                'path': str(img_file),
                'sample_num': sample_num,
                'filename': filename
            })
    
    return subject_images


def calculate_image_similarity(img1_path, img2_path, resize=(224, 224)):
    """
    è®¡ç®—ä¸¤å¼ å›¾åƒçš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨SSIMï¼‰
    
    Args:
        img1_path, img2_path: å›¾åƒè·¯å¾„
        resize: è°ƒæ•´å¤§å°ä»¥åŠ é€Ÿè®¡ç®—
    
    Returns:
        ssim_score: ç»“æ„ç›¸ä¼¼åº¦ (0-1, è¶Šå¤§è¶Šç›¸ä¼¼)
        mse: å‡æ–¹è¯¯å·® (è¶Šå°è¶Šç›¸ä¼¼)
    """
    # è¯»å–å›¾åƒ
    img1 = cv2.imread(img1_path)
    img2 = cv2.imread(img2_path)
    
    if img1 is None or img2 is None:
        return None, None
    
    # è°ƒæ•´å¤§å°
    img1 = cv2.resize(img1, resize)
    img2 = cv2.resize(img2, resize)
    
    # è½¬ä¸ºç°åº¦å›¾
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
    
    # è®¡ç®—SSIM
    ssim_score = ssim(gray1, gray2)
    
    # è®¡ç®—MSE
    mse = np.mean((gray1.astype(float) - gray2.astype(float)) ** 2)
    
    return ssim_score, mse


def analyze_subject_similarities(subject_images, max_subjects=None):
    """
    åˆ†ææ¯ä¸ªå—è¯•è€…çš„å›¾åƒç›¸ä¼¼åº¦
    
    Args:
        subject_images: {subject_id: [image_info]}
        max_subjects: æœ€å¤šåˆ†æçš„å—è¯•è€…æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    
    Returns:
        similarity_stats: ç›¸ä¼¼åº¦ç»Ÿè®¡ä¿¡æ¯
    """
    similarity_results = []
    
    subjects = list(subject_images.keys())
    if max_subjects:
        subjects = subjects[:max_subjects]
    
    print(f"\nå¼€å§‹è®¡ç®—å›¾åƒç›¸ä¼¼åº¦ï¼ˆå…±{len(subjects)}ä¸ªå—è¯•è€…ï¼‰...")
    
    for subject_id in tqdm(subjects):
        images = subject_images[subject_id]
        
        if len(images) < 2:
            continue
        
        # è®¡ç®—è¯¥å—è¯•è€…æ‰€æœ‰å›¾åƒå¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦
        for i in range(len(images)):
            for j in range(i+1, len(images)):
                img1 = images[i]
                img2 = images[j]
                
                ssim_score, mse = calculate_image_similarity(
                    img1['path'], img2['path']
                )
                
                if ssim_score is not None:
                    similarity_results.append({
                        'subject_id': subject_id,
                        'image1': img1['filename'],
                        'image2': img2['filename'],
                        'sample1': img1['sample_num'],
                        'sample2': img2['sample_num'],
                        'ssim': ssim_score,
                        'mse': mse
                    })
    
    return pd.DataFrame(similarity_results)


def print_analysis_summary(subject_images, similarity_df):
    """
    æ‰“å°åˆ†ææ‘˜è¦
    """
    print("\n" + "="*80)
    print("æ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    # 1. åŸºæœ¬ç»Ÿè®¡
    total_subjects = len(subject_images)
    total_images = sum(len(imgs) for imgs in subject_images.values())
    
    print(f"\nã€åŸºæœ¬ç»Ÿè®¡ã€‘")
    print(f"  æ€»å—è¯•è€…æ•°: {total_subjects}")
    print(f"  æ€»å›¾åƒæ•°: {total_images}")
    print(f"  å¹³å‡æ¯äººå›¾åƒæ•°: {total_images / total_subjects:.2f}")
    
    # 2. å›¾åƒæ•°é‡åˆ†å¸ƒ
    images_per_subject = [len(imgs) for imgs in subject_images.values()]
    print(f"\nã€æ¯ä¸ªå—è¯•è€…çš„å›¾åƒæ•°é‡åˆ†å¸ƒã€‘")
    for num_images in sorted(set(images_per_subject)):
        count = images_per_subject.count(num_images)
        percentage = count / total_subjects * 100
        print(f"  {num_images}å¼ å›¾åƒ: {count}äºº ({percentage:.1f}%)")
    
    # 3. æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å—è¯•è€…éƒ½æœ‰3å¼ å›¾
    subjects_with_3_images = sum(1 for count in images_per_subject if count == 3)
    print(f"\nã€å®Œæ•´æ€§æ£€æŸ¥ã€‘")
    print(f"  æœ‰3å¼ å›¾åƒçš„å—è¯•è€…: {subjects_with_3_images} / {total_subjects} ({subjects_with_3_images/total_subjects*100:.1f}%)")
    
    if subjects_with_3_images < total_subjects:
        print(f"  âš ï¸  è­¦å‘Š: æœ‰ {total_subjects - subjects_with_3_images} ä¸ªå—è¯•è€…å›¾åƒæ•°é‡ä¸è¶³3å¼ ")
        
        # åˆ—å‡ºå›¾åƒä¸è¶³çš„å—è¯•è€…
        print(f"\n  å›¾åƒæ•°é‡ä¸è¶³çš„å—è¯•è€…:")
        for subject_id, images in subject_images.items():
            if len(images) != 3:
                print(f"    - {subject_id}: {len(images)}å¼ ")
    
    # 4. ç›¸ä¼¼åº¦åˆ†æ
    if not similarity_df.empty:
        print(f"\nã€ç›¸ä¼¼åº¦åˆ†æã€‘ï¼ˆåŸºäºSSIMï¼ŒèŒƒå›´0-1ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰")
        print(f"  åˆ†æçš„å›¾åƒå¯¹æ•°: {len(similarity_df)}")
        print(f"  å¹³å‡SSIM: {similarity_df['ssim'].mean():.4f}")
        print(f"  ä¸­ä½æ•°SSIM: {similarity_df['ssim'].median():.4f}")
        print(f"  æœ€å°SSIM: {similarity_df['ssim'].min():.4f}")
        print(f"  æœ€å¤§SSIM: {similarity_df['ssim'].max():.4f}")
        print(f"  æ ‡å‡†å·®: {similarity_df['ssim'].std():.4f}")
        
        # SSIMåˆ†å¸ƒ
        print(f"\nã€SSIMåˆ†å¸ƒã€‘")
        bins = [(0, 0.5, "ä½ç›¸ä¼¼åº¦"), (0.5, 0.7, "ä¸­ç­‰ç›¸ä¼¼åº¦"), 
                (0.7, 0.85, "è¾ƒé«˜ç›¸ä¼¼åº¦"), (0.85, 1.0, "æé«˜ç›¸ä¼¼åº¦")]
        for low, high, label in bins:
            count = ((similarity_df['ssim'] >= low) & (similarity_df['ssim'] < high)).sum()
            percentage = count / len(similarity_df) * 100
            print(f"  {label} [{low:.2f}-{high:.2f}): {count} å¯¹ ({percentage:.1f}%)")
        
        # MSEç»Ÿè®¡
        print(f"\nã€MSEç»Ÿè®¡ã€‘ï¼ˆå‡æ–¹è¯¯å·®ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼‰")
        print(f"  å¹³å‡MSE: {similarity_df['mse'].mean():.2f}")
        print(f"  ä¸­ä½æ•°MSE: {similarity_df['mse'].median():.2f}")
        print(f"  æœ€å°MSE: {similarity_df['mse'].min():.2f}")
        print(f"  æœ€å¤§MSE: {similarity_df['mse'].max():.2f}")
    
    # 5. å»ºè®®
    print(f"\nã€æ•°æ®å¤„ç†å»ºè®®ã€‘")
    if not similarity_df.empty:
        avg_ssim = similarity_df['ssim'].mean()
        
        if avg_ssim >= 0.85:
            print(f"  âœ… åŒä¸€å—è¯•è€…çš„å›¾åƒç›¸ä¼¼åº¦å¾ˆé«˜ (SSIM={avg_ssim:.3f})")
            print(f"     å»ºè®®: å¯¹ä¸‰å¼ å›¾åƒå–å¹³å‡åå†åšæ•°æ®å¢å¼º")
            print(f"     ä¼˜ç‚¹: å‡å°‘å™ªå£°ï¼Œæé«˜æ ‡ç­¾è´¨é‡")
        elif avg_ssim >= 0.70:
            print(f"  âš ï¸  åŒä¸€å—è¯•è€…çš„å›¾åƒç›¸ä¼¼åº¦ä¸­ç­‰ (SSIM={avg_ssim:.3f})")
            print(f"     å»ºè®®: å¯ä»¥é€‰æ‹©ä»¥ä¸‹ç­–ç•¥ä¹‹ä¸€:")
            print(f"       1. æ¯å¼ å›¾åƒç‹¬ç«‹åšæ•°æ®å¢å¼ºï¼ˆä¿ç•™å¤šæ ·æ€§ï¼‰")
            print(f"       2. å–å¹³å‡åå†å¢å¼ºï¼ˆå‡å°‘å™ªå£°ï¼‰")
            print(f"     éœ€è¦æ ¹æ®å®é™…æƒ…å†µæƒè¡¡")
        else:
            print(f"  âŒ åŒä¸€å—è¯•è€…çš„å›¾åƒç›¸ä¼¼åº¦è¾ƒä½ (SSIM={avg_ssim:.3f})")
            print(f"     å»ºè®®: æ¯å¼ å›¾åƒç‹¬ç«‹åšæ•°æ®å¢å¼º")
            print(f"     ç†ç”±: å›¾åƒå·®å¼‚å¤§ï¼Œå¯èƒ½æ¥è‡ªä¸åŒä½ç½®/è§’åº¦ï¼Œåº”ä¿ç•™å¤šæ ·æ€§")
    
    print(f"\n  ğŸ”’ æ•°æ®æ³„éœ²é˜²æŠ¤:")
    print(f"     å¿…é¡»æŒ‰å—è¯•è€…IDåˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†")
    print(f"     ç¡®ä¿åŒä¸€å—è¯•è€…çš„æ‰€æœ‰å›¾åƒåœ¨åŒä¸€ä¸ªé›†åˆä¸­")
    
    print("\n" + "="*80)


def save_analysis_results(subject_images, similarity_df, output_dir='./analysis_results'):
    """
    ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ä¿å­˜æ¯ä¸ªå—è¯•è€…çš„å›¾åƒåˆ—è¡¨
    subject_summary = []
    for subject_id, images in subject_images.items():
        subject_summary.append({
            'subject_id': subject_id,
            'num_images': len(images),
            'image_files': [img['filename'] for img in images]
        })
    
    subject_df = pd.DataFrame(subject_summary)
    subject_df.to_csv(f'{output_dir}/subject_summary.csv', index=False)
    print(f"\nâœ… å—è¯•è€…ç»Ÿè®¡å·²ä¿å­˜åˆ°: {output_dir}/subject_summary.csv")
    
    # 2. ä¿å­˜ç›¸ä¼¼åº¦ç»“æœ
    if not similarity_df.empty:
        similarity_df.to_csv(f'{output_dir}/similarity_analysis.csv', index=False)
        print(f"âœ… ç›¸ä¼¼åº¦åˆ†æå·²ä¿å­˜åˆ°: {output_dir}/similarity_analysis.csv")
        
        # 3. ä¿å­˜ç»Ÿè®¡æ‘˜è¦
        with open(f'{output_dir}/analysis_summary.txt', 'w', encoding='utf-8') as f:
            f.write(f"æ•°æ®é›†ç›¸ä¼¼åº¦ç»Ÿè®¡\n")
            f.write(f"="*60 + "\n\n")
            f.write(f"æ€»å—è¯•è€…æ•°: {len(subject_images)}\n")
            f.write(f"æ€»å›¾åƒæ•°: {sum(len(imgs) for imgs in subject_images.values())}\n")
            f.write(f"åˆ†æçš„å›¾åƒå¯¹æ•°: {len(similarity_df)}\n\n")
            f.write(f"SSIMç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡å€¼: {similarity_df['ssim'].mean():.4f}\n")
            f.write(f"  ä¸­ä½æ•°: {similarity_df['ssim'].median():.4f}\n")
            f.write(f"  æ ‡å‡†å·®: {similarity_df['ssim'].std():.4f}\n")
            f.write(f"  èŒƒå›´: [{similarity_df['ssim'].min():.4f}, {similarity_df['ssim'].max():.4f}]\n")
        
        print(f"âœ… ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {output_dir}/analysis_summary.txt")


if __name__ == '__main__':
    # é…ç½®è·¯å¾„
    image_dir = '/home/szdx/LNX/data/TA/Healthy/Images'
    output_dir = './analysis_results'
    
    print("å¼€å§‹åˆ†ææ•°æ®é›†...")
    print(f"å›¾åƒç›®å½•: {image_dir}\n")
    
    # æ­¥éª¤1: åˆ†æå›¾åƒåˆ†å¸ƒ
    print("æ­¥éª¤1: ç»Ÿè®¡æ¯ä¸ªå—è¯•è€…çš„å›¾åƒæ•°é‡...")
    subject_images = analyze_image_distribution(image_dir)
    
    # æ­¥éª¤2: è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå¯ä»¥è®¾ç½®max_subjectsé™åˆ¶åˆ†ææ•°é‡ä»¥åŠ é€Ÿï¼‰
    # å¦‚æœè¦åˆ†ææ‰€æœ‰å—è¯•è€…ï¼Œå°†max_subjects=None
    # å¦‚æœåªæƒ³å¿«é€Ÿæµ‹è¯•ï¼Œå¯ä»¥è®¾ç½®max_subjects=50
    print("\næ­¥éª¤2: è®¡ç®—å›¾åƒç›¸ä¼¼åº¦...")
    print("æç¤º: è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")
    similarity_df = analyze_subject_similarities(subject_images, max_subjects=None)
    
    # æ­¥éª¤3: æ‰“å°åˆ†ææ‘˜è¦
    print_analysis_summary(subject_images, similarity_df)
    
    # æ­¥éª¤4: ä¿å­˜ç»“æœ
    save_analysis_results(subject_images, similarity_df, output_dir)
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
