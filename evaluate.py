"""
è¯„ä¼°è„šæœ¬
"""
import argparse
import os
import torch
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from tqdm import tqdm
import json

from dataset import load_dataset
from model import get_model


def evaluate(model, test_loader, device, image_paths=None):
    """è¯„ä¼°æ¨¡å‹
    
    Args:
        model: å¾…è¯„ä¼°æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        image_paths: å¯é€‰ï¼Œå›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œç”¨äºè¿½è¸ªæ¯ä¸ªæ ·æœ¬
    
    Returns:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        all_preds: é¢„æµ‹å€¼æ•°ç»„
        all_targets: çœŸå®å€¼æ•°ç»„
        filenames: æ–‡ä»¶ååˆ—è¡¨ï¼ˆå¦‚æœæä¾›äº†image_pathsï¼‰
    """
    model.eval()
    
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for images, ages in tqdm(test_loader, desc='Evaluating'):
            images = images.to(device)
            outputs = model(images)
            
            all_preds.extend(outputs.cpu().numpy())
            all_targets.extend(ages.numpy())
    
    all_preds = np.array(all_preds)
    all_targets = np.array(all_targets)
    
    # è®¡ç®—æŒ‡æ ‡
    mae = np.mean(np.abs(all_preds - all_targets))
    rmse = np.sqrt(np.mean((all_preds - all_targets) ** 2))
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(all_preds, all_targets)[0, 1]
    
    # è®¡ç®—åœ¨Nå¹´è¯¯å·®èŒƒå›´å†…çš„å‡†ç¡®ç‡
    errors = np.abs(all_preds - all_targets)
    acc_5 = np.mean(errors <= 5) * 100
    acc_10 = np.mean(errors <= 10) * 100
    acc_15 = np.mean(errors <= 15) * 100
    
    metrics = {
        'MAE': float(mae),
        'RMSE': float(rmse),
        'Correlation': float(correlation),
        'Accuracy_5years': float(acc_5),
        'Accuracy_10years': float(acc_10),
        'Accuracy_15years': float(acc_15),
        'predictions': [float(x) for x in all_preds.tolist()],
        'targets': [float(x) for x in all_targets.tolist()]
    }
    
    # æå–æ–‡ä»¶å
    filenames = None
    if image_paths is not None:
        filenames = [Path(p).name for p in image_paths]
    
    return metrics, all_preds, all_targets, filenames


def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
    import matplotlib.font_manager as fm
    
    # å°è¯•å¤šç§ä¸­æ–‡å­—ä½“
    chinese_fonts = [
        'WenQuanYi Micro Hei',
        'WenQuanYi Zen Hei', 
        'Noto Sans CJK SC',
        'Noto Sans CJK',
        'Source Han Sans CN',
        'SimHei',
        'Microsoft YaHei',
        'AR PL UMing CN',
        'DejaVu Sans'  # fallback
    ]
    
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in chinese_fonts:
        if font in available_fonts:
            plt.rcParams['font.sans-serif'] = [font]
            plt.rcParams['axes.unicode_minus'] = False
            print(f'ä½¿ç”¨å­—ä½“: {font}')
            return True
    
    # å¦‚æœæ²¡æœ‰ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡
    print('è­¦å‘Š: æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾')
    return False


def calculate_age_group_mae(predictions, targets, bin_width=10):
    """è®¡ç®—æ¯ä¸ªå¹´é¾„æ®µçš„MAE"""
    predictions = np.array(predictions).flatten()
    targets = np.array(targets).flatten()
    
    # ç¡®å®šå¹´é¾„èŒƒå›´
    min_age = int(np.floor(targets.min() / bin_width) * bin_width)
    max_age = int(np.ceil(targets.max() / bin_width) * bin_width)
    
    age_group_results = []
    
    for start_age in range(min_age, max_age, bin_width):
        end_age = start_age + bin_width
        mask = (targets >= start_age) & (targets < end_age)
        
        if mask.sum() > 0:
            group_preds = predictions[mask]
            group_targets = targets[mask]
            group_mae = np.mean(np.abs(group_preds - group_targets))
            group_rmse = np.sqrt(np.mean((group_preds - group_targets) ** 2))
            
            age_group_results.append({
                'age_range': f'{start_age}-{end_age}',
                'start_age': start_age,
                'end_age': end_age,
                'count': int(mask.sum()),
                'mae': float(group_mae),
                'rmse': float(group_rmse),
                'mean_true_age': float(group_targets.mean()),
                'mean_pred_age': float(group_preds.mean()),
                'true_mean': float(group_targets.mean()),  # æ·»åŠ ç”¨äºç»˜å›¾
                'pred_mean': float(group_preds.mean())     # æ·»åŠ ç”¨äºç»˜å›¾
            })
    
    return age_group_results


def save_error_analysis(predictions, targets, filenames, output_dir, top_n=50, mae_threshold=None):
    """ä¿å­˜è¯¯å·®åˆ†æç»“æœ
    
    ä¿å­˜è¯¯å·®æœ€å¤§å’Œè¯¯å·®æœ€å°çš„æ ·æœ¬åˆ—è¡¨åˆ°txtæ–‡ä»¶ï¼Œä¾¿äºåç»­åˆ†æã€‚
    
    Args:
        predictions: é¢„æµ‹å€¼æ•°ç»„
        targets: çœŸå®å€¼æ•°ç»„
        filenames: æ–‡ä»¶ååˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        top_n: ä¿å­˜å‰Nä¸ªæœ€å¤§/æœ€å°è¯¯å·®çš„æ ·æœ¬ï¼ˆé»˜è®¤50ï¼‰
        mae_threshold: å¯é€‰ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼çš„æ ·æœ¬è§†ä¸ºå¼‚å¸¸å¤§
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è¯¯å·®
    errors = np.abs(predictions.flatten() - targets.flatten())
    signed_errors = predictions.flatten() - targets.flatten()  # æœ‰ç¬¦å·è¯¯å·®
    
    # åˆ›å»ºæ ·æœ¬ä¿¡æ¯åˆ—è¡¨
    samples = []
    for i, (fname, pred, target, err, signed_err) in enumerate(zip(
            filenames, predictions.flatten(), targets.flatten(), errors, signed_errors)):
        samples.append({
            'filename': fname,
            'true_age': target,
            'pred_age': pred,
            'mae': err,
            'error': signed_err  # æ­£å€¼=é¢„æµ‹åå¤§, è´Ÿå€¼=é¢„æµ‹åå°
        })
    
    # æŒ‰MAEé™åºæ’åˆ—
    samples_sorted = sorted(samples, key=lambda x: x['mae'], reverse=True)
    
    # è®¡ç®—å¼‚å¸¸é˜ˆå€¼
    if mae_threshold is None:
        mae_threshold = errors.mean() + 2 * errors.std()
    outlier_count = sum(1 for s in samples if s['mae'] > mae_threshold)
    
    # ä¿å­˜é«˜è¯¯å·®æ ·æœ¬ï¼ˆè¯¯å·®æœ€å¤§çš„å‰Nä¸ªï¼Œå¼‚å¸¸æ ·æœ¬ç”¨âš ï¸æ ‡è®°ï¼‰
    high_error_path = output_dir / 'high_error_samples.txt'
    with open(high_error_path, 'w', encoding='utf-8') as f:
        f.write(f"# é«˜è¯¯å·®æ ·æœ¬åˆ—è¡¨ï¼ˆæŒ‰MAEé™åºæ’åˆ—ï¼‰\n")
        f.write(f"# æ€»æ ·æœ¬æ•°: {len(samples)}, æ˜¾ç¤ºå‰{min(top_n, len(samples))}ä¸ª\n")
        f.write(f"# å¹³å‡MAE: {errors.mean():.2f}å², æ ‡å‡†å·®: {errors.std():.2f}å²\n")
        f.write(f"# å¼‚å¸¸é˜ˆå€¼: {mae_threshold:.2f}å² (å‡å€¼+2Ã—æ ‡å‡†å·®), å¼‚å¸¸æ ·æœ¬æ•°: {outlier_count} ({100*outlier_count/len(samples):.1f}%)\n")
        f.write(f"# ----------------------------------------------------------------\n")
        f.write(f"# æ–‡ä»¶å | çœŸå®å¹´é¾„ | é¢„æµ‹å¹´é¾„ | MAE | è¯¯å·®æ–¹å‘ | å¼‚å¸¸æ ‡è®°\n")
        f.write(f"# ----------------------------------------------------------------\n\n")
        
        for sample in samples_sorted[:top_n]:
            direction = "åå¤§" if sample['error'] > 0 else "åå°"
            outlier_flag = "âš ï¸å¼‚å¸¸" if sample['mae'] > mae_threshold else ""
            f.write(f"{sample['filename']}\t"
                   f"{sample['true_age']:.1f}\t"
                   f"{sample['pred_age']:.1f}\t"
                   f"{sample['mae']:.2f}\t"
                   f"{direction}({sample['error']:+.2f})\t"
                   f"{outlier_flag}\n")
    
    print(f'é«˜è¯¯å·®æ ·æœ¬å·²ä¿å­˜: {high_error_path}')
    print(f'  - åŒ…å« {outlier_count} ä¸ªå¼‚å¸¸æ ·æœ¬ï¼ˆMAE > {mae_threshold:.2f}å²ï¼‰')
    
    # ä¿å­˜ä½è¯¯å·®æ ·æœ¬ï¼ˆè¯¯å·®æœ€å°çš„å‰Nä¸ªï¼‰
    low_error_path = output_dir / 'low_error_samples.txt'
    with open(low_error_path, 'w', encoding='utf-8') as f:
        f.write(f"# ä½è¯¯å·®æ ·æœ¬åˆ—è¡¨ï¼ˆæŒ‰MAEå‡åºæ’åˆ—ï¼‰\n")
        f.write(f"# æ€»æ ·æœ¬æ•°: {len(samples)}, æ˜¾ç¤ºå‰{min(top_n, len(samples))}ä¸ª\n")
        f.write(f"# å¹³å‡MAE: {errors.mean():.2f}å², æ ‡å‡†å·®: {errors.std():.2f}å²\n")
        f.write(f"# ----------------------------------------------------------------\n")
        f.write(f"# æ–‡ä»¶å | çœŸå®å¹´é¾„ | é¢„æµ‹å¹´é¾„ | MAE | è¯¯å·®æ–¹å‘\n")
        f.write(f"# ----------------------------------------------------------------\n\n")
        
        # å‡åºæ’åˆ—ï¼ˆè¯¯å·®æœ€å°çš„ï¼‰
        samples_low = sorted(samples, key=lambda x: x['mae'])
        for sample in samples_low[:top_n]:
            direction = "åå¤§" if sample['error'] > 0 else "åå°"
            f.write(f"{sample['filename']}\t"
                   f"{sample['true_age']:.1f}\t"
                   f"{sample['pred_age']:.1f}\t"
                   f"{sample['mae']:.2f}\t"
                   f"{direction}({sample['error']:+.2f})\n")
    
    print(f'ä½è¯¯å·®æ ·æœ¬å·²ä¿å­˜: {low_error_path}')
    
    # è®¡ç®—å¼‚å¸¸é˜ˆå€¼ä½†ä¸å†å•ç‹¬ç”Ÿæˆoutlieræ–‡ä»¶ï¼Œè€Œæ˜¯åœ¨é«˜è¯¯å·®æ–‡ä»¶ä¸­æ ‡è®°
    if mae_threshold is None:
        mae_threshold = errors.mean() + 2 * errors.std()  # é»˜è®¤ä½¿ç”¨2ä¸ªæ ‡å‡†å·®
    
    outlier_count = sum(1 for s in samples if s['mae'] > mae_threshold)
    print(f'å¼‚å¸¸æ ·æœ¬ç»Ÿè®¡: å…±{outlier_count}ä¸ª ({100*outlier_count/len(samples):.1f}%), é˜ˆå€¼={mae_threshold:.2f}å²')
    print(f'  (å¼‚å¸¸æ ·æœ¬å·²åœ¨ high_error_samples.txt ä¸­ç”¨âš ï¸æ ‡è®°)')
    
    # è¿”å›æ ·æœ¬æ•°æ®ä¾›åç»­ç‰¹å¾åˆ†æä½¿ç”¨
    return {
        'high_error_samples': samples_sorted[:top_n],
        'low_error_samples': samples_low[:top_n],
        'all_samples': samples,
        'outlier_threshold': float(mae_threshold),
        'outlier_count': outlier_count,
        'mean_mae': float(errors.mean()),
        'std_mae': float(errors.std())
    }


def analyze_image_features(sample_info, image_dir, output_dir):
    """
    åˆ†æé«˜é”™è¯¯å’Œä½é”™è¯¯æ ·æœ¬çš„å›¾åƒç‰¹å¾å·®å¼‚
    å¦‚æœå‘ç°æ˜æ˜¾å·®å¼‚ï¼Œå¯è€ƒè™‘ä½¿ç”¨ç›´æ–¹å›¾åŒ¹é…ç­‰é¢„å¤„ç†æ–¹æ³•
    
    Args:
        sample_info: save_error_analysisè¿”å›çš„æ ·æœ¬ä¿¡æ¯å­—å…¸
        image_dir: å›¾åƒæ–‡ä»¶ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
    """
    try:
        import cv2
    except ImportError:
        print('è­¦å‘Š: æœªå®‰è£…opencv-pythonï¼Œè·³è¿‡å›¾åƒç‰¹å¾åˆ†æ')
        return None
    
    output_dir = Path(output_dir)
    high_samples = sample_info['high_error_samples']
    low_samples = sample_info['low_error_samples']
    
    def compute_image_stats(filename):
        """è®¡ç®—å•å¼ å›¾åƒçš„ç»Ÿè®¡ç‰¹å¾"""
        # é€’å½’æœç´¢å›¾åƒ
        img_path = None
        for root, dirs, files in os.walk(image_dir):
            if filename in files:
                img_path = os.path.join(root, filename)
                break
        
        if img_path is None or not os.path.exists(img_path):
            return None
        
        try:
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                return None
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            mean_intensity = float(np.mean(img))
            std_intensity = float(np.std(img))
            
            # æ¸…æ™°åº¦ (Laplacianæ–¹å·®)
            laplacian = cv2.Laplacian(img, cv2.CV_64F)
            clarity = float(np.var(laplacian))
            
            # å¯¹æ¯”åº¦
            contrast = std_intensity / mean_intensity if mean_intensity > 0 else 0
            
            # ååº¦
            skewness = float(np.mean(((img - mean_intensity) / std_intensity) ** 3)) if std_intensity > 0 else 0
            
            # ç›´æ–¹å›¾ç»Ÿè®¡
            hist, _ = np.histogram(img, bins=256, range=(0, 256))
            hist = hist / hist.sum()  # å½’ä¸€åŒ–
            entropy = -np.sum(hist * np.log(hist + 1e-10))  # ç†µ
            
            return {
                'mean': mean_intensity,
                'std': std_intensity,
                'clarity': clarity,
                'contrast': contrast,
                'skewness': skewness,
                'entropy': float(entropy)
            }
        except Exception as e:
            return None
    
    # è®¡ç®—é«˜é”™è¯¯æ ·æœ¬çš„ç‰¹å¾
    print('\næ­£åœ¨è®¡ç®—é«˜é”™è¯¯æ ·æœ¬çš„å›¾åƒç‰¹å¾...')
    high_features = []
    for sample in high_samples:
        stats = compute_image_stats(sample['filename'])
        if stats:
            high_features.append(stats)
    
    # è®¡ç®—ä½é”™è¯¯æ ·æœ¬çš„ç‰¹å¾
    print('æ­£åœ¨è®¡ç®—ä½é”™è¯¯æ ·æœ¬çš„å›¾åƒç‰¹å¾...')
    low_features = []
    for sample in low_samples:
        stats = compute_image_stats(sample['filename'])
        if stats:
            low_features.append(stats)
    
    if len(high_features) == 0 or len(low_features) == 0:
        print('è­¦å‘Š: æ— æ³•è®¡ç®—è¶³å¤Ÿçš„å›¾åƒç‰¹å¾')
        return None
    
    # ç»Ÿè®¡åˆ†æ
    feature_names = ['mean', 'std', 'clarity', 'contrast', 'skewness', 'entropy']
    
    summary_path = output_dir / 'image_feature_analysis.txt'
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("# å›¾åƒç‰¹å¾åˆ†æï¼šé«˜é”™è¯¯æ ·æœ¬ vs ä½é”™è¯¯æ ·æœ¬\n")
        f.write(f"# åˆ†ææ ·æœ¬æ•°: é«˜é”™è¯¯={len(high_features)}, ä½é”™è¯¯={len(low_features)}\n")
        f.write("# ================================================================\n\n")
        
        f.write("## ç»Ÿè®¡æ‘˜è¦\n\n")
        f.write(f"{'ç‰¹å¾':<15} | {'é«˜é”™è¯¯(å‡å€¼Â±æ ‡å‡†å·®)':<25} | {'ä½é”™è¯¯(å‡å€¼Â±æ ‡å‡†å·®)':<25} | å·®å¼‚ç™¾åˆ†æ¯”\n")
        f.write("-" * 95 + "\n")
        
        comparison_results = {}
        for feat in feature_names:
            high_vals = [f[feat] for f in high_features]
            low_vals = [f[feat] for f in low_features]
            
            high_mean = np.mean(high_vals)
            high_std = np.std(high_vals)
            low_mean = np.mean(low_vals)
            low_std = np.std(low_vals)
            
            # è®¡ç®—å·®å¼‚ç™¾åˆ†æ¯”
            diff_pct = abs(high_mean - low_mean) / low_mean * 100 if low_mean != 0 else 0
            
            comparison_results[feat] = {
                'high_mean': high_mean,
                'high_std': high_std,
                'low_mean': low_mean,
                'low_std': low_std,
                'diff_pct': diff_pct
            }
            
            f.write(f"{feat:<15} | {high_mean:>10.2f} Â± {high_std:<10.2f} | "
                   f"{low_mean:>10.2f} Â± {low_std:<10.2f} | {diff_pct:>8.1f}%\n")
        
        f.write("\n\n## åˆ†æç»“è®º\n\n")
        
        # è¯†åˆ«æ˜¾è‘—å·®å¼‚çš„ç‰¹å¾
        significant_features = [(k, v['diff_pct']) for k, v in comparison_results.items() 
                               if v['diff_pct'] > 10]  # å·®å¼‚è¶…è¿‡10%è§†ä¸ºæ˜¾è‘—
        
        if significant_features:
            f.write("å‘ç°ä»¥ä¸‹ç‰¹å¾å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ˆ>10%ï¼‰:\n")
            for feat, diff in sorted(significant_features, key=lambda x: x[1], reverse=True):
                f.write(f"  - {feat}: {diff:.1f}%\n")
            
            f.write("\nğŸ’¡ å»ºè®®:\n")
            f.write("  1. å›¾åƒç‰¹å¾å­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½\n")
            f.write("  2. è€ƒè™‘ä½¿ç”¨ä»¥ä¸‹é¢„å¤„ç†æ–¹æ³•:\n")
            
            if any(feat in ['mean', 'std', 'contrast'] for feat, _ in significant_features):
                f.write("     - ç›´æ–¹å›¾å‡è¡¡åŒ– (CLAHE)\n")
                f.write("     - å¯¹æ¯”åº¦å½’ä¸€åŒ–\n")
            
            if any(feat == 'clarity' for feat, _ in significant_features):
                f.write("     - é”åŒ–æ»¤æ³¢\n")
                f.write("     - æ•°æ®å¢å¼ºï¼ˆæ¨¡ç³Š/å»å™ªï¼‰\n")
            
            f.write("  3. å‚è€ƒä½é”™è¯¯æ ·æœ¬çš„å›¾åƒé£æ ¼è¿›è¡Œç›´æ–¹å›¾åŒ¹é…\n")
        else:
            f.write("æœªå‘ç°æ˜¾è‘—çš„å›¾åƒç‰¹å¾å·®å¼‚ï¼ˆ<10%ï¼‰\n")
            f.write("å›¾åƒè´¨é‡å¯èƒ½ä¸æ˜¯ä¸»è¦å½±å“å› ç´ ï¼Œå»ºè®®ä»æ¨¡å‹æ¶æ„æˆ–æ ‡æ³¨è´¨é‡æ–¹é¢åˆ†æã€‚\n")
    
    print(f'å›¾åƒç‰¹å¾åˆ†æå·²ä¿å­˜: {summary_path}')
    
    # è¿”å›æ¯”è¾ƒç»“æœä¾›åç»­ä½¿ç”¨
    return {
        'high_features': high_features,
        'low_features': low_features,
        'comparison': comparison_results,
        'significant_features': significant_features if significant_features else []
    }


def plot_results(predictions, targets, output_dir, use_chinese=True):
    """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ ·å¼
    sns.set_style('whitegrid')
    has_chinese = setup_chinese_font() if use_chinese else False
    
    # 1. é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # å·¦å›¾ï¼šæ•£ç‚¹å›¾ + å›å½’çº¿
    ax = axes[0]
    ax.scatter(targets, predictions, alpha=0.5, s=30)
    
    # ç»˜åˆ¶å®Œç¾é¢„æµ‹çº¿
    min_val = min(targets.min(), predictions.min())
    max_val = max(targets.max(), predictions.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
    
    # æ‹Ÿåˆçº¿æ€§å›å½’
    z = np.polyfit(targets, predictions, 1)
    p = np.poly1d(z)
    ax.plot(targets, p(targets), 'g-', lw=2, label=f'Fit: y={z[0]:.2f}x+{z[1]:.2f}')
    
    # æ ‡ç­¾æ–‡å­—
    if has_chinese:
        ax.set_xlabel('çœŸå®å¹´é¾„ (å²)', fontsize=12)
        ax.set_ylabel('é¢„æµ‹å¹´é¾„ (å²)', fontsize=12)
        ax.set_title('é¢„æµ‹å¹´é¾„ vs çœŸå®å¹´é¾„', fontsize=14, fontweight='bold')
    else:
        ax.set_xlabel('True Age (years)', fontsize=12)
        ax.set_ylabel('Predicted Age (years)', fontsize=12)
        ax.set_title('Predicted vs True Age', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šè¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
    ax = axes[1]
    errors = predictions - targets
    ax.hist(errors, bins=30, edgecolor='black', alpha=0.7)
    ax.axvline(0, color='r', linestyle='--', lw=2, label='Zero Error')
    ax.axvline(errors.mean(), color='g', linestyle='--', lw=2, 
               label=f'Mean Error: {errors.mean():.2f}')
    if has_chinese:
        ax.set_xlabel('é¢„æµ‹è¯¯å·® (å²)', fontsize=12)
        ax.set_ylabel('é¢‘æ•°', fontsize=12)
        ax.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    else:
        ax.set_xlabel('Prediction Error (years)', fontsize=12)
        ax.set_ylabel('Frequency', fontsize=12)
        ax.set_title('Error Distribution', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'evaluation_results.png', dpi=300, bbox_inches='tight')
    print(f'å›¾è¡¨å·²ä¿å­˜: {output_dir / "evaluation_results.png"}')
    plt.close()
    
    # 2. Bland-Altmanå›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    mean_age = (predictions + targets) / 2
    diff_age = predictions - targets
    
    ax.scatter(mean_age, diff_age, alpha=0.5, s=30)
    ax.axhline(0, color='r', linestyle='-', lw=2, label='Mean Difference')
    ax.axhline(diff_age.mean(), color='g', linestyle='--', lw=2, 
               label=f'Bias: {diff_age.mean():.2f}')
    ax.axhline(diff_age.mean() + 1.96 * diff_age.std(), color='orange', 
               linestyle='--', lw=2, label=f'Â±1.96 SD')
    ax.axhline(diff_age.mean() - 1.96 * diff_age.std(), color='orange', 
               linestyle='--', lw=2)
    
    if has_chinese:
        ax.set_xlabel('å¹³å‡å¹´é¾„ (å²)', fontsize=12)
        ax.set_ylabel('å·®å¼‚ (é¢„æµ‹ - çœŸå®, å²)', fontsize=12)
        ax.set_title('Bland-Altmanå›¾', fontsize=14, fontweight='bold')
    else:
        ax.set_xlabel('Mean Age (years)', fontsize=12)
        ax.set_ylabel('Difference (Predicted - True, years)', fontsize=12)
        ax.set_title('Bland-Altman Plot', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'bland_altman.png', dpi=300, bbox_inches='tight')
    print(f'å›¾è¡¨å·²ä¿å­˜: {output_dir / "bland_altman.png"}')
    plt.close()
    
    # 3. å¹´é¾„åˆ†æ®µï¼šçœŸå®å¹´é¾„ vs é¢„æµ‹å¹´é¾„å¯¹æ¯”æŸ±çŠ¶å›¾
    age_group_mae = calculate_age_group_mae(predictions, targets, bin_width=10)
    
    fig, ax = plt.subplots(figsize=(14, 7))
    x = range(len(age_group_mae))
    labels = [g['age_range'] for g in age_group_mae]
    true_means = [g['true_mean'] for g in age_group_mae]
    pred_means = [g['pred_mean'] for g in age_group_mae]
    counts = [g['count'] for g in age_group_mae]
    
    # åˆ†ç»„æŸ±çŠ¶å›¾ï¼šçœŸå®å¹´é¾„ vs é¢„æµ‹å¹´é¾„
    width = 0.35
    x_pos = np.arange(len(labels))
    
    bars1 = ax.bar(x_pos - width/2, true_means, width, 
                   label='çœŸå®å¹´é¾„' if has_chinese else 'True Age',
                   color='steelblue', alpha=0.8, edgecolor='black')
    bars2 = ax.bar(x_pos + width/2, pred_means, width,
                   label='é¢„æµ‹å¹´é¾„' if has_chinese else 'Predicted Age', 
                   color='coral', alpha=0.8, edgecolor='black')
    
    # æ ‡æ³¨æ ·æœ¬æ•°é‡
    for i, (true_m, pred_m, count) in enumerate(zip(true_means, pred_means, counts)):
        max_height = max(true_m, pred_m)
        ax.text(i, max_height + 1.5, f'n={count}',
                ha='center', va='bottom', fontsize=9, color='gray')
    
    # æ·»åŠ å‚è€ƒçº¿ï¼ˆå®Œç¾é¢„æµ‹çº¿ï¼‰
    ax.plot([-0.5, len(labels)-0.5], [-0.5, len(labels)-0.5],
            'g--', alpha=0.3, linewidth=1.5, label='å®Œç¾é¢„æµ‹' if has_chinese else 'Perfect Prediction')
    
    ax.set_xticks(x_pos)
    ax.set_xticklabels(labels, rotation=45, ha='right')
    if has_chinese:
        ax.set_xlabel('å¹´é¾„æ®µ (å²)', fontsize=12)
        ax.set_ylabel('å¹³å‡å¹´é¾„ (å²)', fontsize=12)
        ax.set_title('å„å¹´é¾„æ®µçœŸå®å¹´é¾„ vs é¢„æµ‹å¹´é¾„å¯¹æ¯”', fontsize=14, fontweight='bold')
    else:
        ax.set_xlabel('Age Group (years)', fontsize=12)
        ax.set_ylabel('Mean Age (years)', fontsize=12)
        ax.set_title('True vs Predicted Age by Age Group', fontsize=14, fontweight='bold')
    
    ax.legend(loc='upper left', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'age_group_comparison.png', dpi=300, bbox_inches='tight')
    print(f'å›¾è¡¨å·²ä¿å­˜: {output_dir / "age_group_comparison.png"}')
    plt.close()
    
    return age_group_mae


def get_run_name_from_checkpoint(checkpoint_path):
    """ä»checkpointè·¯å¾„æå–è¿è¡Œåç§°
    
    æ”¯æŒçš„è·¯å¾„æ ¼å¼:
    - outputs/run_20260108_115437/best_model.pth -> run_20260108_115437
    - outputs/ablation/01_baseline/run_20260108_115437/best_model.pth -> 01_baseline_run_20260108_115437
    """
    checkpoint_path = Path(checkpoint_path)
    parent = checkpoint_path.parent
    
    # æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦æ˜¯ run_xxx æ ¼å¼
    if parent.name.startswith('run_'):
        # æ£€æŸ¥ä¸Šä¸€çº§æ˜¯å¦æ˜¯ ablation çš„å­ç›®å½•ï¼ˆå¦‚ 01_baselineï¼‰
        grandparent = parent.parent
        if grandparent.name != 'outputs' and grandparent.name != 'ablation':
            # è¿™æ˜¯ ablation çš„å­ç›®å½•ï¼Œå¦‚ 01_baseline
            return f"{grandparent.name}_{parent.name}"
        else:
            return parent.name
    else:
        # ç›´æ¥è¿”å›çˆ¶ç›®å½•å
        return parent.name


def main(args):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'ä½¿ç”¨è®¾å¤‡: {device}')
    
    # å…ˆåŠ è½½checkpointè·å–è®­ç»ƒæ—¶çš„é…ç½®ï¼ˆé˜²æ­¢æ•°æ®æ³„æ¼ï¼‰
    print(f'åŠ è½½æ¨¡å‹é…ç½®: {args.checkpoint}')
    checkpoint = torch.load(args.checkpoint, map_location=device, weights_only=False)
    train_args = checkpoint.get('args', {})
    
    # ä½¿ç”¨è®­ç»ƒæ—¶çš„åˆ’åˆ†å‚æ•°ï¼Œç¡®ä¿æ•°æ®åˆ’åˆ†ä¸€è‡´
    test_size = train_args.get('test_size', args.test_size)
    val_size = train_args.get('val_size', args.val_size)
    seed = train_args.get('seed', args.seed)
    use_age_stratify = train_args.get('use_age_stratify', True)  # é»˜è®¤ä½¿ç”¨åˆ†å±‚æŠ½æ ·
    age_bin_width = train_args.get('age_bin_width', 10)
    min_age = train_args.get('min_age', args.min_age)
    max_age = train_args.get('max_age', args.max_age)
    
    print(f'\nâš ï¸  ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æ•°æ®åˆ’åˆ†å‚æ•°ï¼ˆé˜²æ­¢æ•°æ®æ³„æ¼ï¼‰:')
    print(f'    test_size={test_size}, val_size={val_size}, seed={seed}')
    print(f'    use_age_stratify={use_age_stratify}, age_bin_width={age_bin_width}')
    print(f'    age_range={min_age}-{max_age}')
    
    # åŠ è½½æ•°æ®
    print('\nåŠ è½½æ•°æ®é›†...')
    _, _, test_dataset = load_dataset(
        args.image_dir, 
        args.excel_path,
        test_size=test_size,
        val_size=val_size,
        random_state=seed,
        use_age_stratify=use_age_stratify,
        age_bin_width=age_bin_width,
        min_age=min_age,
        max_age=max_age
    )
    
    # è·å–æµ‹è¯•é›†çš„image_pathsç”¨äºè¯¯å·®åˆ†æ
    test_image_paths = test_dataset.image_paths
    
    # åˆ›å»ºDataLoader
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model_name = train_args.get('model', 'resnet50')
    dropout = train_args.get('dropout', 0.5)
    
    print(f'\nåˆ›å»ºæ¨¡å‹: {model_name}, dropout={dropout}')
    model = get_model(model_name, pretrained=False, dropout=dropout)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    
    print(f'æ¨¡å‹è®­ç»ƒè½®æ¬¡: {checkpoint["epoch"]}')
    print(f'éªŒè¯é›†MAE: {checkpoint["val_mae"]:.2f} years')
    
    # è¯„ä¼°ï¼ˆä¼ å…¥image_pathsè·å–æ–‡ä»¶åï¼‰
    print('\nå¼€å§‹è¯„ä¼°...')
    metrics, predictions, targets, filenames = evaluate(model, test_loader, device, test_image_paths)
    
    # æ‰“å°ç»“æœ
    print('\n' + '='*60)
    print('æµ‹è¯•é›†è¯„ä¼°ç»“æœ')
    print('='*60)
    print(f'MAE (å¹³å‡ç»å¯¹è¯¯å·®):       {metrics["MAE"]:.2f} å²')
    print(f'RMSE (å‡æ–¹æ ¹è¯¯å·®):        {metrics["RMSE"]:.2f} å²')
    print(f'ç›¸å…³ç³»æ•°:                 {metrics["Correlation"]:.4f}')
    print(f'Â±5å¹´å†…å‡†ç¡®ç‡:            {metrics["Accuracy_5years"]:.1f}%')
    print(f'Â±10å¹´å†…å‡†ç¡®ç‡:           {metrics["Accuracy_10years"]:.1f}%')
    print(f'Â±15å¹´å†…å‡†ç¡®ç‡:           {metrics["Accuracy_15years"]:.1f}%')
    print('='*60)
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output_dir == './evaluation_results':
        # å¦‚æœä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•ï¼Œåˆ™è‡ªåŠ¨ä»checkpointè·¯å¾„æ¨æ–­
        run_name = get_run_name_from_checkpoint(args.checkpoint)
        output_dir = Path('./evaluation_results') / run_name
        print(f'\nè‡ªåŠ¨è®¾ç½®è¾“å‡ºç›®å½•: {output_dir}')
    else:
        output_dir = Path(args.output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¡ç®—å¹´é¾„åˆ†æ®µMAE
    age_group_mae = calculate_age_group_mae(predictions, targets, bin_width=10)
    
    # æ‰“å°å¹´é¾„åˆ†æ®µç»“æœ
    print('\nå„å¹´é¾„æ®µMAE:')
    print('-' * 50)
    for group in age_group_mae:
        print(f"  {group['age_range']:>8}å²: MAE={group['mae']:.2f}, RMSE={group['rmse']:.2f}, n={group['count']}")
    print('-' * 50)
    
    # æ·»åŠ å¹´é¾„åˆ†æ®µMAEåˆ°metrics
    metrics['age_group_mae'] = age_group_mae
    
    # ç§»é™¤predictionså’Œtargetsä»¥å‡å°æ–‡ä»¶å¤§å°
    metrics_save = {k: v for k, v in metrics.items() if k not in ['predictions', 'targets']}
    metrics_save['total_samples'] = len(predictions)
    
    with open(output_dir / 'test_metrics.json', 'w', encoding='utf-8') as f:
        json.dump(metrics_save, f, indent=2, ensure_ascii=False)
    print(f'\næŒ‡æ ‡å·²ä¿å­˜: {output_dir / "test_metrics.json"}')
    
    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    predictions_data = {
        'predictions': [float(x) for x in predictions.tolist()],
        'targets': [float(x) for x in targets.tolist()],
        'filenames': filenames if filenames else []
    }
    with open(output_dir / 'predictions.json', 'w') as f:
        json.dump(predictions_data, f)
    print(f'é¢„æµ‹è¯¦æƒ…å·²ä¿å­˜: {output_dir / "predictions.json"}')
    
    # ä¿å­˜è¯¯å·®åˆ†æç»“æœï¼ˆé«˜è¯¯å·®/ä½è¯¯å·®æ ·æœ¬åˆ—è¡¨ï¼‰
    if filenames:
        print('\nè¿›è¡Œè¯¯å·®åˆ†æ...')
        error_analysis = save_error_analysis(
            predictions, targets, filenames, output_dir,
            top_n=args.top_n
        )
        # å°†è¯¯å·®åˆ†æç»Ÿè®¡æ·»åŠ åˆ°metrics
        metrics_save['error_analysis'] = {
            'outlier_threshold': error_analysis['outlier_threshold'],
            'outlier_count': error_analysis['outlier_count'],
            'mean_mae': error_analysis['mean_mae'],
            'std_mae': error_analysis['std_mae']
        }
        # æ›´æ–°ä¿å­˜çš„metrics
        with open(output_dir / 'test_metrics.json', 'w', encoding='utf-8') as f:
            json.dump(metrics_save, f, indent=2, ensure_ascii=False)
        
        # åˆ†æå›¾åƒç‰¹å¾å·®å¼‚
        print('\nåˆ†æé«˜é”™è¯¯/ä½é”™è¯¯æ ·æœ¬çš„å›¾åƒç‰¹å¾å·®å¼‚...')
        feature_analysis = analyze_image_features(
            error_analysis, args.image_dir, output_dir
        )
        if feature_analysis and feature_analysis['significant_features']:
            print(f"\nâš ï¸  å‘ç° {len(feature_analysis['significant_features'])} ä¸ªæ˜¾è‘—å·®å¼‚çš„å›¾åƒç‰¹å¾")
            print("   è¯¦è§: image_feature_analysis.txt")
    
    # ç»˜åˆ¶å›¾è¡¨
    print('\nç»˜åˆ¶ç»“æœå›¾è¡¨...')
    plot_results(predictions, targets, output_dir)
    
    print('\nè¯„ä¼°å®Œæˆ!')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='è¯„ä¼°TAè¶…å£°å›¾åƒå¹´é¾„é¢„æµ‹æ¨¡å‹')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--image-dir', type=str, 
                       default='/home/szdx/LNX/data/TA/Healthy/Images',
                       help='å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--excel-path', type=str,
                       default='/home/szdx/LNX/data/TA/characteristics.xlsx',
                       help='Excelæ ‡ç­¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--checkpoint', type=str, 
                       default='./outputs/best_model.pth',
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--output-dir', type=str, 
                       default='./evaluation_results',
                       help='è¾“å‡ºç›®å½•')
    
    # æ•°æ®é›†åˆ’åˆ†ï¼ˆé»˜è®¤ä»checkpointè‡ªåŠ¨è¯»å–ï¼Œä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
    parser.add_argument('--test-size', type=float, default=0.15, help='æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ä»checkpointè¯»å–ï¼‰')
    parser.add_argument('--val-size', type=float, default=0.15, help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ä»checkpointè¯»å–ï¼‰')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­ï¼ˆé»˜è®¤ä»checkpointè¯»å–ï¼‰')
    parser.add_argument('--min-age', type=float, default=0, help='æœ€å°å¹´é¾„ï¼ˆé»˜è®¤ä»checkpointè¯»å–ï¼‰')
    parser.add_argument('--max-age', type=float, default=100, help='æœ€å¤§å¹´é¾„ï¼ˆé»˜è®¤ä»checkpointè¯»å–ï¼‰')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num-workers', type=int, default=4, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--top-n', type=int, default=50, help='ä¿å­˜è¯¯å·®æœ€å¤§/æœ€å°çš„å‰Nä¸ªæ ·æœ¬ï¼ˆé»˜è®¤50ï¼‰')
    
    args = parser.parse_args()
    
    main(args)
